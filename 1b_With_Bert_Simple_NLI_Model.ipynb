{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "1b With Bert - Simple NLI Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antgr/pytorch-nli/blob/master/1b_With_Bert_Simple_NLI_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKxZsM3u-xdN",
        "colab_type": "code",
        "outputId": "56a00897-7d96-49a9-b3dc-c6575cee937d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        }
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-vp0dk5aj\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-vp0dk5aj\n",
            "Requirement already satisfied (use --upgrade to upgrade): transformers==2.1.1 from git+https://github.com/huggingface/transformers in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (1.17.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (1.10.14)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (4.28.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (2019.11.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (0.1.83)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (0.0.35)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.14 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1) (1.13.14)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (2.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (0.14.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->transformers==2.1.1) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->transformers==2.1.1) (0.15.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.1.1-cp36-none-any.whl size=327209 sha256=4416d4d6a54a25db70598d6528e9c90fb5492cd73b343078ec61c35d25ede81c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vz51xvm8/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "Successfully built transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF8yCJe7-rWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Us38WvF_vLm",
        "colab_type": "code",
        "outputId": "139d6ea1-b10c-4ee2-9acd-5e6165b8c255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY__buAC_zc9",
        "colab_type": "code",
        "outputId": "b019c7a0-6c78-4273-de0e-bdfa4269405c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tokenizer.vocab)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6x4U_-8_4_l",
        "colab_type": "code",
        "outputId": "49186703-6a9b-4dfa-a03b-7172c7afc7e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "print(init_token, eos_token, pad_token, unk_token)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] [SEP] [PAD] [UNK]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeuWQcD-ACl4",
        "colab_type": "code",
        "outputId": "b6de25ab-a0de-4108-b0d5-40632ae3cbc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101 102 0 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBnqSqDXAGo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "print(max_input_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5HbCU5lAI02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP7uVkykAI5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import data\n",
        "\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "\n",
        "LABEL = data.LabelField(dtype = torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42VsMRDnFr6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcqcU0Wf-rWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data, test_data = datasets.SNLI.splits(TEXT, LABEL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MMwYcWv-rWb",
        "colab_type": "code",
        "outputId": "10bef5b3-3ee1-45e9-d66d-6df1b8e61887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 549367\n",
            "Number of validation examples: 9842\n",
            "Number of testing examples: 9824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQXDOBNlaVsv",
        "colab_type": "code",
        "outputId": "ff1f0e58-5698-41c3-f153-9ee64c4e9acf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vars(train_data.examples[6])['hypothesis']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1996, 2879, 17260, 2015, 2091, 1996, 11996, 1012]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a7SFokQIVn-",
        "colab_type": "code",
        "outputId": "6b4b8f7a-347c-479a-f564-84b760dcc6f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vars(train_data.examples[6])['hypothesis']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1996, 2879, 17260, 2015, 2091, 1996, 11996, 1012]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoJfHQ6DJbm-",
        "colab_type": "code",
        "outputId": "08c75a2a-00cc-4245-9e69-119fb704467b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "vars(train_data.examples[6])['premise']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1037,\n",
              " 2879,\n",
              " 2003,\n",
              " 8660,\n",
              " 2006,\n",
              " 17260,\n",
              " 6277,\n",
              " 1999,\n",
              " 1996,\n",
              " 2690,\n",
              " 1997,\n",
              " 1037,\n",
              " 2417,\n",
              " 2958,\n",
              " 1012]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxSja88TJdwW",
        "colab_type": "code",
        "outputId": "a3d4922b-d1d7-4ec9-e37d-4d0c3db545fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vars(train_data.examples[6])['label']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'contradiction'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UA-Vqg7FzhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCMeCIApF3MI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(LABEL.vocab.itos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxQC4bBHOsoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycIxPE2X-rWf",
        "colab_type": "code",
        "outputId": "ca6af291-94b0-4d6b-d2b6-21573959d184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'premise': [1037, 2711, 2006, 1037, 3586, 14523, 2058, 1037, 3714, 2091, 13297, 1012], 'hypothesis': [1037, 2711, 2003, 2731, 2010, 3586, 2005, 1037, 2971, 1012], 'label': 'neutral'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDuNX0nC-rWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4tTp2Hn-rWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtCKwsgoSoGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Eai1qlAf8CP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLISum(nn.Module):\n",
        "    def __init__(self, \n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 fc_layers,\n",
        "                 output_dim, \n",
        "                 dropout,\n",
        "                 PAD_IDX):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = bert\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        #print (\"self.embedding_dim: \", self.embedding_dim)\n",
        "\n",
        "        self.translation = nn.Linear(self.embedding_dim, self.hidden_dim)\n",
        "        #print (\"self.translation: \", self.translation)\n",
        "        \n",
        "        fcs = [nn.Linear(self.hidden_dim*2 , self.hidden_dim*2 ) for _ in range(fc_layers)]\n",
        "        \n",
        "        self.fcs = nn.ModuleList(fcs)\n",
        "        #print (\"self.fcs: \", self.fcs)\n",
        "        \n",
        "        self.fc_out = nn.Linear(self.hidden_dim*2 , output_dim)\n",
        "        #print (\"self.fc_out: \", self.fc_out)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, prem, hypo):\n",
        "        with torch.no_grad():\n",
        "            embedded_prem = bert(prem)[0]\n",
        "            #print (\"embedded_prem: \", embedded_prem.shape)\n",
        "            embedded_hypo = bert(hypo)[0]\n",
        "            #print (\"embedded_hypo: \", embedded_hypo.shape)\n",
        "\n",
        "        embedded_prem = embedded_prem.sum(dim = 1)\n",
        "        #print (\"embedded_prem: \", embedded_prem.shape)\n",
        "        embedded_hypo = embedded_hypo.sum(dim = 1)\n",
        "        #print (\"embedded_hypo: \", embedded_hypo.shape)\n",
        "\n",
        "        #prem = [prem sent len, batch size]\n",
        "        #hypo = [hypo sent len, batch size]\n",
        "        \n",
        "        translated_prem = F.relu(self.translation(embedded_prem))\n",
        "        #print (\"translated_prem: \", translated_prem.shape)\n",
        "        translated_hypo = F.relu(self.translation(embedded_hypo))\n",
        "        #print (\"translated_hypo: \", translated_hypo.shape)\n",
        "        \n",
        "        #translated_prem = [prem sent len, batch size, hidden dim]\n",
        "        #translated_hypo = [hypo sent len, batch size, hidden dim]\n",
        "\n",
        "        hidden = torch.cat((translated_prem, translated_hypo), dim = 1)\n",
        "        #print (\"hidden: \", hidden.shape)\n",
        "\n",
        "        #hidden = [batch size, hid dim * 2]\n",
        "            \n",
        "        for fc in self.fcs:\n",
        "            hidden = fc(hidden)\n",
        "            hidden = F.relu(hidden)\n",
        "            hidden = self.dropout(hidden)\n",
        "        #print (\"hidden: \", hidden.shape)\n",
        "        prediction = self.fc_out(hidden)\n",
        "        #print (\"prediction: \", prediction.shape)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "      \n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2gxQYDU-rW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLIRNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 fc_layers,\n",
        "                 output_dim, \n",
        "                 dropout,\n",
        "                 PAD_IDX):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = bert\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        #print (\"self.embedding_dim: \", self.embedding_dim)\n",
        "        \n",
        "        self.translation = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        #print (\"self.translation: \", self.translation)\n",
        "\n",
        "        self.rnn = nn.LSTM(self.embedding_dim, self.hidden_dim)\n",
        "        #print (\"self.rnn: \", self.rnn)\n",
        "        \n",
        "        fcs = [nn.Linear(self.hidden_dim*2 , self.hidden_dim*2 ) for _ in range(fc_layers)]\n",
        "        \n",
        "        self.fcs = nn.ModuleList(fcs)\n",
        "        #print (\"self.fcs: \", self.fcs)\n",
        "        \n",
        "        self.fc_out = nn.Linear(self.hidden_dim*2 , output_dim)\n",
        "        #print (\"self.fc_out: \", self.fc_out)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, prem, hypo):\n",
        "        with torch.no_grad():\n",
        "            embedded_prem = bert(prem)[0]\n",
        "            #print (\"embedded_prem: \", embedded_prem.shape)\n",
        "            embedded_hypo = bert(hypo)[0]\n",
        "            #print (\"embedded_hypo: \", embedded_hypo.shape)\n",
        "\n",
        "        embedded_prem = embedded_prem.permute(1,0,2)\n",
        "        #print (\"embedded_prem: \", embedded_prem.shape)\n",
        "        embedded_hypo = embedded_hypo.permute(1,0,2)\n",
        "        #print (\"embedded_hypo: \", embedded_hypo.shape)\n",
        "\n",
        "        _, (hidden_prem, _) = self.rnn(embedded_prem)\n",
        "        #print (\"hidden_prem: \", hidden_prem.shape)\n",
        "        _, (hidden_prem, _) = self.rnn(embedded_hypo)\n",
        "        #print (\"hidden_prem: \", hidden_prem.shape)\n",
        "\n",
        "        embedded_prem = hidden_prem.squeeze(0)\n",
        "        #print (\"embedded_prem: \", embedded_prem.shape)\n",
        "        embedded_hypo = hidden_prem.squeeze(0)\n",
        "        #print (\"embedded_hypo: \", embedded_hypo.shape)\n",
        "\n",
        "        #prem = [prem sent len, batch size]\n",
        "        #hypo = [hypo sent len, batch size]\n",
        "        \n",
        "\n",
        "        translated_prem = F.relu(self.translation(embedded_prem))\n",
        "        #print (\"translated_prem: \", translated_prem.shape)\n",
        "        translated_hypo = F.relu(self.translation(embedded_hypo))\n",
        "        #print (\"translated_hypo: \", translated_hypo.shape)\n",
        "        \n",
        "        #translated_prem = [prem sent len, batch size, hidden dim]\n",
        "        #translated_hypo = [hypo sent len, batch size, hidden dim]\n",
        "\n",
        "        hidden = torch.cat((translated_prem, translated_hypo), dim = 1)\n",
        "        #print (\"hidden: \", hidden.shape)\n",
        "\n",
        "        #hidden = [batch size, hid dim * 2]\n",
        "            \n",
        "        for fc in self.fcs:\n",
        "            hidden = fc(hidden)\n",
        "            hidden = F.relu(hidden)\n",
        "            hidden = self.dropout(hidden)\n",
        "        #print (\"hidden: \", hidden.shape)\n",
        "        prediction = self.fc_out(hidden)\n",
        "        #print (\"prediction: \", prediction.shape)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "      \n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntyN9ysemSQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLISumRNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 fc_layers,\n",
        "                 output_dim, \n",
        "                 dropout,\n",
        "                 PAD_IDX):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = bert\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        #print (\"self.embedding_dim: \", self.embedding_dim)\n",
        "        \n",
        "        self.translation1 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        #print (\"self.translation1: \", self.translation1)\n",
        "\n",
        "        self.translation = nn.Linear(self.embedding_dim, self.hidden_dim)\n",
        "        #print (\"self.translation: \", self.translation)\n",
        "\n",
        "        self.rnn = nn.LSTM(self.embedding_dim, self.hidden_dim)\n",
        "        #print (\"self.rnn: \", self.rnn)\n",
        "        \n",
        "        fcs = [nn.Linear(self.hidden_dim*4 , self.hidden_dim*4 ) for _ in range(fc_layers)]\n",
        "        \n",
        "        self.fcs = nn.ModuleList(fcs)\n",
        "        #print (\"self.fcs: \", self.fcs)\n",
        "        \n",
        "        self.fc_out = nn.Linear(self.hidden_dim*4 , output_dim)\n",
        "        #print (\"self.fc_out: \", self.fc_out)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, prem, hypo):\n",
        "        with torch.no_grad():\n",
        "            embedded_prem = bert(prem)[0]\n",
        "            #print (\"embedded_prem: \", embedded_prem.shape)\n",
        "            embedded_hypo = bert(hypo)[0]\n",
        "            #print (\"embedded_hypo: \", embedded_hypo.shape)\n",
        "\n",
        "        embedded_prem1 = embedded_prem.permute(1,0,2)\n",
        "        #print (\"embedded_prem1: \", embedded_prem1.shape)\n",
        "        embedded_hypo1 = embedded_hypo.permute(1,0,2)\n",
        "        #print (\"embedded_hypo1: \", embedded_hypo1.shape)\n",
        "\n",
        "        _, (hidden_prem, _) = self.rnn(embedded_prem1)\n",
        "        #print (\"hidden_prem1: \", hidden_prem.shape)\n",
        "        _, (hidden_prem, _) = self.rnn(embedded_hypo1)\n",
        "        #print (\"hidden_prem1: \", hidden_prem.shape)\n",
        "\n",
        "        embedded_prem = embedded_prem.sum(dim=1)\n",
        "        #print (\"embedded_prem: \", embedded_prem.shape)\n",
        "        embedded_hypo = embedded_hypo.sum(dim=1)\n",
        "        #print (\"embedded_hypo: \", embedded_hypo.shape)\n",
        "\n",
        "        embedded_prem1 = hidden_prem.squeeze(0)\n",
        "        #print (\"embedded_prem1: \", embedded_prem1.shape)\n",
        "        embedded_hypo1 = hidden_prem.squeeze(0)\n",
        "        #print (\"embedded_hypo1: \", embedded_hypo1.shape)\n",
        "\n",
        "        #prem = [prem sent len, batch size]\n",
        "        #hypo = [hypo sent len, batch size]\n",
        "        \n",
        "\n",
        "        translated_prem1 = F.relu(self.translation1(embedded_prem1))\n",
        "        #print (\"translated_prem1: \", translated_prem1.shape)\n",
        "        translated_hypo1 = F.relu(self.translation1(embedded_hypo1))\n",
        "        #print (\"translated_hypo1: \", translated_hypo1.shape)\n",
        "        \n",
        "        #translated_prem = [prem sent len, batch size, hidden dim]\n",
        "        #translated_hypo = [hypo sent len, batch size, hidden dim]\n",
        "\n",
        "        translated_prem = F.relu(self.translation(embedded_prem))\n",
        "        #print (\"translated_prem: \", translated_prem.shape)\n",
        "        translated_hypo = F.relu(self.translation(embedded_hypo))\n",
        "        #print (\"translated_hypo: \", translated_hypo.shape)\n",
        "\n",
        "\n",
        "        hidden = torch.cat((translated_prem, translated_hypo), dim = 1)\n",
        "        #print (\"hidden: \", hidden.shape)\n",
        "\n",
        "        #hidden = [batch size, hid dim * 2]\n",
        "\n",
        "        hidden1 = torch.cat((translated_prem1, translated_hypo1), dim = 1)\n",
        "\n",
        "        hidden2 = torch.cat((hidden, hidden1), dim = 1)\n",
        "            \n",
        "        for fc in self.fcs:\n",
        "            hidden2 = fc(hidden2)\n",
        "            hidden2 = F.relu(hidden2)\n",
        "            hidden2 = self.dropout(hidden2)\n",
        "        #print (\"hidden: \", hidden.shape)\n",
        "        prediction = self.fc_out(hidden2)\n",
        "        #print (\"prediction: \", prediction.shape)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "      \n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM0-vYdc-rW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_DIM = 300\n",
        "FC_LAYERS = 3\n",
        "OUTPUT_DIM = len(LABEL.vocab)\n",
        "DROPOUT = 0.25\n",
        "PAD_IDX = pad_token_idx\n",
        "\n",
        "model = NLISumRNN(bert,\n",
        "              HIDDEN_DIM,\n",
        "               FC_LAYERS,\n",
        "               OUTPUT_DIM,\n",
        "               DROPOUT,\n",
        "               PAD_IDX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHkeCgaQc9xY",
        "colab_type": "code",
        "outputId": "647251cb-4cf3-4457-d03f-be5ebb10a508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 115,414,443 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlqWvF6c-rW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plKK48D4eY7b",
        "colab_type": "code",
        "outputId": "0a3fddaf-81ad-48f0-ff3a-26eee3d71987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 5,932,203 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akr6SY9OfhCV",
        "colab_type": "code",
        "outputId": "1cddccf1-5d6f-4f33-83c5-34c5c46382dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "translation1.weight\n",
            "translation1.bias\n",
            "translation.weight\n",
            "translation.bias\n",
            "rnn.weight_ih_l0\n",
            "rnn.weight_hh_l0\n",
            "rnn.bias_ih_l0\n",
            "rnn.bias_hh_l0\n",
            "fcs.0.weight\n",
            "fcs.0.bias\n",
            "fcs.1.weight\n",
            "fcs.1.bias\n",
            "fcs.2.weight\n",
            "fcs.2.bias\n",
            "fc_out.weight\n",
            "fc_out.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy9lMDrJ-rXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mmNfj57-rXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvD-m9ib-rXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbltTB_o-rXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaGHJEUJ-rXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        prem = batch.premise\n",
        "        hypo = batch.hypothesis\n",
        "        labels = batch.label\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #prem = [prem sent len, batch size]\n",
        "        #hypo = [hypo sent len, batch size]\n",
        "        \n",
        "        predictions = model(prem, hypo)\n",
        "        \n",
        "        #predictions = [batch size, output dim]\n",
        "        #labels = [batch size]\n",
        "        \n",
        "        loss = criterion(predictions, labels)\n",
        "                \n",
        "        acc = categorical_accuracy(predictions, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjeB4J-7-rXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            prem = batch.premise\n",
        "            hypo = batch.hypothesis\n",
        "            labels = batch.label\n",
        "                        \n",
        "            predictions = model(prem, hypo)\n",
        "            \n",
        "            loss = criterion(predictions, labels)\n",
        "                \n",
        "            acc = categorical_accuracy(predictions, labels)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwJMZ19T-rXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVuIygUp-rXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPOCHS = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaDrOm3l-rXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fIakIy9-rXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}