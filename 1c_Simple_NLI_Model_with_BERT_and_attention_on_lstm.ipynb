{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "1c - Simple NLI Model with BERT and attention on lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antgr/pytorch-nli/blob/master/1c_Simple_NLI_Model_with_BERT_and_attention_on_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKxZsM3u-xdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF8yCJe7-rWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Us38WvF_vLm",
        "colab_type": "code",
        "outputId": "7f5da375-1b93-445b-b983-5fad21b5152b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY__buAC_zc9",
        "colab_type": "code",
        "outputId": "7d90eb37-849d-4a03-b707-ac9b9894f10c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tokenizer.vocab)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6x4U_-8_4_l",
        "colab_type": "code",
        "outputId": "4da73c95-0b57-43c0-e310-969f409f6414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "print(init_token, eos_token, pad_token, unk_token)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] [SEP] [PAD] [UNK]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeuWQcD-ACl4",
        "colab_type": "code",
        "outputId": "367a8f4f-8bef-4f5f-ec74-9ea8708f8c50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101 102 0 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBnqSqDXAGo9",
        "colab_type": "code",
        "outputId": "f8ff1b5f-b06a-494b-e159-97cc46fa1b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "print(max_input_length)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5HbCU5lAI02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP7uVkykAI5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import data\n",
        "\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "\n",
        "LABEL = data.LabelField(dtype = torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42VsMRDnFr6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcqcU0Wf-rWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data, test_data = datasets.SNLI.splits(TEXT, LABEL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MMwYcWv-rWb",
        "colab_type": "code",
        "outputId": "f3af1b54-9ae6-4d55-f103-d70ee1ae6686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 549367\n",
            "Number of validation examples: 9842\n",
            "Number of testing examples: 9824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQXDOBNlaVsv",
        "colab_type": "code",
        "outputId": "ff1f0e58-5698-41c3-f153-9ee64c4e9acf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vars(train_data.examples[6])['hypothesis']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1996, 2879, 17260, 2015, 2091, 1996, 11996, 1012]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a7SFokQIVn-",
        "colab_type": "code",
        "outputId": "6b4b8f7a-347c-479a-f564-84b760dcc6f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vars(train_data.examples[6])['hypothesis']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1996, 2879, 17260, 2015, 2091, 1996, 11996, 1012]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoJfHQ6DJbm-",
        "colab_type": "code",
        "outputId": "08c75a2a-00cc-4245-9e69-119fb704467b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "vars(train_data.examples[6])['premise']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1037,\n",
              " 2879,\n",
              " 2003,\n",
              " 8660,\n",
              " 2006,\n",
              " 17260,\n",
              " 6277,\n",
              " 1999,\n",
              " 1996,\n",
              " 2690,\n",
              " 1997,\n",
              " 1037,\n",
              " 2417,\n",
              " 2958,\n",
              " 1012]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxSja88TJdwW",
        "colab_type": "code",
        "outputId": "a3d4922b-d1d7-4ec9-e37d-4d0c3db545fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vars(train_data.examples[6])['label']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'contradiction'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UA-Vqg7FzhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCMeCIApF3MI",
        "colab_type": "code",
        "outputId": "cc806385-6d8a-4c01-f37b-ff1ced251989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(LABEL.vocab.itos)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['entailment', 'contradiction', 'neutral']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxQC4bBHOsoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycIxPE2X-rWf",
        "colab_type": "code",
        "outputId": "ca6af291-94b0-4d6b-d2b6-21573959d184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'premise': [1037, 2711, 2006, 1037, 3586, 14523, 2058, 1037, 3714, 2091, 13297, 1012], 'hypothesis': [1037, 2711, 2003, 2731, 2010, 3586, 2005, 1037, 2971, 1012], 'label': 'neutral'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDuNX0nC-rWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4tTp2Hn-rWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtCKwsgoSoGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywRsiY2MlqoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#adapted from https://github.com/littleflow3r/attention-bilstm-for-relation-classification/blob/master/model.py\n",
        "class attbilstm(nn.Module):\n",
        "    def __init__(self, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        #BATCH_SIZE = 512\n",
        "        self.hidden_dim = hid_dim\n",
        "        #self.batch_size = BATCH_SIZE\n",
        "        self.emb_dim = emb_dim\n",
        "        \n",
        "        self.encoder = nn.LSTM(self.emb_dim, self.hidden_dim, num_layers=2, bidirectional=True, dropout=0.1)\n",
        "        #print (\"self.encoder: \", self.encoder)\n",
        "        self.fc = nn.Linear(self.hidden_dim,  self.hidden_dim)\n",
        "        #print (\"self.fc: \", self.fc)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        #print (\"self.dropout \", self.dropout)\n",
        "    \n",
        "    def attnetwork(self, encoder_out, final_hidden):\n",
        "        hidden = final_hidden.squeeze(0)\n",
        "        #print (\"hidden \", hidden.shape)\n",
        "        attn_weights = torch.bmm(encoder_out, hidden.unsqueeze(2)).squeeze(2)\n",
        "        #print (\"attn_weights \", attn_weights.shape)\n",
        "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
        "        #print (\"soft_attn_weights \", soft_attn_weights.shape)\n",
        "        new_hidden = torch.bmm(encoder_out.transpose(1,2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
        "        #print (\"new_hidden \", new_hidden.shape)\n",
        "\n",
        "        return new_hidden\n",
        "    \n",
        "    def forward(self, sequence):\n",
        "        emb_input = sequence\n",
        "        #print (\"emb_input \", emb_input.shape)\n",
        "        inputx = self.dropout(emb_input)\n",
        "        #print (\"inputx \", inputx.shape)\n",
        "        output, (hn, cn) = self.encoder(inputx)\n",
        "        #print (\"hn \", hn.shape)\n",
        "\n",
        "        #bidirectional=True is a precondition for this:\n",
        "        fbout = output[:, :, :self.hidden_dim]+ output[:, :, self.hidden_dim:]\n",
        "        #print (\"fbout \", fbout.shape)\n",
        "        fbout = fbout.permute(1,0,2)\n",
        "        #print (\"fbout \", fbout.shape)\n",
        "        fbhn = (hn[-2,:,:]+hn[-1,:,:]).unsqueeze(0)\n",
        "        #print (\"fbhn \", fbhn.shape)\n",
        "        #print (fbhn.shape, fbout.shape)\n",
        "        attn_out = self.attnetwork(fbout, fbhn)\n",
        "        #print (\"attn_out \", attn_out.shape)\n",
        "        #attn1_out = self.attnetwork1(output, hn)\n",
        "        logits = self.fc(attn_out)\n",
        "        #print (\"logits \", logits.shape)\n",
        "\n",
        "        return output, (logits, cn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Eai1qlAf8CP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLISum(nn.Module):\n",
        "    def __init__(self, \n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 fc_layers,\n",
        "                 output_dim, \n",
        "                 dropout,\n",
        "                 PAD_IDX):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = bert\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        #print (\"self.embedding_dim: \", self.embedding_dim)\n",
        "\n",
        "        self.translation = nn.Linear(self.embedding_dim, self.hidden_dim)\n",
        "        #print (\"self.translation: \", self.translation)\n",
        "        \n",
        "        fcs = [nn.Linear(self.hidden_dim*2 , self.hidden_dim*2 ) for _ in range(fc_layers)]\n",
        "        \n",
        "        self.fcs = nn.ModuleList(fcs)\n",
        "        #print (\"self.fcs: \", self.fcs)\n",
        "        \n",
        "        self.fc_out = nn.Linear(self.hidden_dim*2 , output_dim)\n",
        "        #print (\"self.fc_out: \", self.fc_out)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, prem, hypo):\n",
        "        with torch.no_grad():\n",
        "            embedded_prem = bert(prem)[0]\n",
        "            #print (\"embedded_prem: \", embedded_prem.shape)\n",
        "            embedded_hypo = bert(hypo)[0]\n",
        "            #print (\"embedded_hypo: \", embedded_hypo.shape)\n",
        "\n",
        "        embedded_prem = embedded_prem.sum(dim = 1)\n",
        "        #print (\"embedded_prem: \", embedded_prem.shape)\n",
        "        embedded_hypo = embedded_hypo.sum(dim = 1)\n",
        "        #print (\"embedded_hypo: \", embedded_hypo.shape)\n",
        "\n",
        "        #prem = [prem sent len, batch size]\n",
        "        #hypo = [hypo sent len, batch size]\n",
        "        \n",
        "        translated_prem = F.relu(self.translation(embedded_prem))\n",
        "        #print (\"translated_prem: \", translated_prem.shape)\n",
        "        translated_hypo = F.relu(self.translation(embedded_hypo))\n",
        "        #print (\"translated_hypo: \", translated_hypo.shape)\n",
        "        \n",
        "        #translated_prem = [prem sent len, batch size, hidden dim]\n",
        "        #translated_hypo = [hypo sent len, batch size, hidden dim]\n",
        "\n",
        "        hidden = torch.cat((translated_prem, translated_hypo), dim = 1)\n",
        "        #print (\"hidden: \", hidden.shape)\n",
        "\n",
        "        #hidden = [batch size, hid dim * 2]\n",
        "            \n",
        "        for fc in self.fcs:\n",
        "            hidden = fc(hidden)\n",
        "            hidden = F.relu(hidden)\n",
        "            hidden = self.dropout(hidden)\n",
        "        #print (\"hidden: \", hidden.shape)\n",
        "        prediction = self.fc_out(hidden)\n",
        "        #print (\"prediction: \", prediction.shape)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "      \n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2gxQYDU-rW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLIRNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 fc_layers,\n",
        "                 output_dim, \n",
        "                 dropout,\n",
        "                 PAD_IDX):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = bert\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        #print (\"self.embedding_dim: \", self.embedding_dim)\n",
        "        \n",
        "        self.translation = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        #print (\"self.translation: \", self.translation)\n",
        "\n",
        "        self.rnn = nn.LSTM(self.embedding_dim, self.hidden_dim)\n",
        "        #print (\"self.rnn: \", self.rnn)\n",
        "        \n",
        "        fcs = [nn.Linear(self.hidden_dim*2 , self.hidden_dim*2 ) for _ in range(fc_layers)]\n",
        "        \n",
        "        self.fcs = nn.ModuleList(fcs)\n",
        "        #print (\"self.fcs: \", self.fcs)\n",
        "        \n",
        "        self.fc_out = nn.Linear(self.hidden_dim*2 , output_dim)\n",
        "        #print (\"self.fc_out: \", self.fc_out)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, prem, hypo):\n",
        "        with torch.no_grad():\n",
        "            embedded_prem = bert(prem)[0]\n",
        "            #print (\"embedded_prem: \", embedded_prem.shape)\n",
        "            embedded_hypo = bert(hypo)[0]\n",
        "            #print (\"embedded_hypo: \", embedded_hypo.shape)\n",
        "\n",
        "        embedded_prem = embedded_prem.permute(1,0,2)\n",
        "        #print (\"embedded_prem: \", embedded_prem.shape)\n",
        "        embedded_hypo = embedded_hypo.permute(1,0,2)\n",
        "        #print (\"embedded_hypo: \", embedded_hypo.shape)\n",
        "\n",
        "        _, (hidden_prem, _) = self.rnn(embedded_prem)\n",
        "        #print (\"hidden_prem: \", hidden_prem.shape)\n",
        "        _, (hidden_prem, _) = self.rnn(embedded_hypo)\n",
        "        #print (\"hidden_prem: \", hidden_prem.shape)\n",
        "\n",
        "        embedded_prem = hidden_prem.squeeze(0)\n",
        "        #print (\"embedded_prem: \", embedded_prem.shape)\n",
        "        embedded_hypo = hidden_prem.squeeze(0)\n",
        "        #print (\"embedded_hypo: \", embedded_hypo.shape)\n",
        "\n",
        "        #prem = [prem sent len, batch size]\n",
        "        #hypo = [hypo sent len, batch size]\n",
        "        \n",
        "\n",
        "        translated_prem = F.relu(self.translation(embedded_prem))\n",
        "        #print (\"translated_prem: \", translated_prem.shape)\n",
        "        translated_hypo = F.relu(self.translation(embedded_hypo))\n",
        "        #print (\"translated_hypo: \", translated_hypo.shape)\n",
        "        \n",
        "        #translated_prem = [prem sent len, batch size, hidden dim]\n",
        "        #translated_hypo = [hypo sent len, batch size, hidden dim]\n",
        "\n",
        "        hidden = torch.cat((translated_prem, translated_hypo), dim = 1)\n",
        "        #print (\"hidden: \", hidden.shape)\n",
        "\n",
        "        #hidden = [batch size, hid dim * 2]\n",
        "            \n",
        "        for fc in self.fcs:\n",
        "            hidden = fc(hidden)\n",
        "            hidden = F.relu(hidden)\n",
        "            hidden = self.dropout(hidden)\n",
        "        #print (\"hidden: \", hidden.shape)\n",
        "        prediction = self.fc_out(hidden)\n",
        "        #print (\"prediction: \", prediction.shape)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "      \n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntyN9ysemSQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLISumRNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 fc_layers,\n",
        "                 output_dim, \n",
        "                 dropout,\n",
        "                 PAD_IDX):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = bert\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        #print (\"self.embedding_dim: \", self.embedding_dim)\n",
        "        \n",
        "        self.translation1 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        #print (\"self.translation1: \", self.translation1)\n",
        "\n",
        "        self.translation = nn.Linear(self.embedding_dim, self.hidden_dim)\n",
        "        #print (\"self.translation: \", self.translation)\n",
        "\n",
        "        self.rnn = attbilstm(self.embedding_dim, self.hidden_dim)\n",
        "        #print (\"self.rnn: \", self.rnn)\n",
        "        \n",
        "        fcs = [nn.Linear(self.hidden_dim*4 , self.hidden_dim*4 ) for _ in range(fc_layers)]\n",
        "        \n",
        "        self.fcs = nn.ModuleList(fcs)\n",
        "        #print (\"self.fcs: \", self.fcs)\n",
        "        \n",
        "        self.fc_out = nn.Linear(self.hidden_dim*4 , output_dim)\n",
        "        #print (\"self.fc_out: \", self.fc_out)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, prem, hypo):\n",
        "        with torch.no_grad():\n",
        "            embedded_prem = bert(prem)[0]\n",
        "            #print (\"embedded_prem: \", embedded_prem.shape)\n",
        "            embedded_hypo = bert(hypo)[0]\n",
        "            #print (\"embedded_hypo: \", embedded_hypo.shape)\n",
        "\n",
        "        embedded_prem1 = embedded_prem.permute(1,0,2)\n",
        "        #print (\"embedded_prem1: \", embedded_prem1.shape)\n",
        "        embedded_hypo1 = embedded_hypo.permute(1,0,2)\n",
        "        #print (\"embedded_hypo1: \", embedded_hypo1.shape)\n",
        "\n",
        "        _, (hidden_prem, _) = self.rnn(embedded_prem1)\n",
        "        #print (\"hidden_prem1: \", hidden_prem.shape)\n",
        "        _, (hidden_prem, _) = self.rnn(embedded_hypo1)\n",
        "        #print (\"hidden_prem1: \", hidden_prem.shape)\n",
        "\n",
        "        embedded_prem = embedded_prem.sum(dim=1)\n",
        "        #print (\"embedded_prem: \", embedded_prem.shape)\n",
        "        embedded_hypo = embedded_hypo.sum(dim=1)\n",
        "        #print (\"embedded_hypo: \", embedded_hypo.shape)\n",
        "\n",
        "        embedded_prem1 = hidden_prem.squeeze(0)\n",
        "        #print (\"embedded_prem1: \", embedded_prem1.shape)\n",
        "        embedded_hypo1 = hidden_prem.squeeze(0)\n",
        "        #print (\"embedded_hypo1: \", embedded_hypo1.shape)\n",
        "\n",
        "        #prem = [prem sent len, batch size]\n",
        "        #hypo = [hypo sent len, batch size]\n",
        "        \n",
        "\n",
        "        translated_prem1 = F.relu(self.translation1(embedded_prem1))\n",
        "        #print (\"translated_prem1: \", translated_prem1.shape)\n",
        "        translated_hypo1 = F.relu(self.translation1(embedded_hypo1))\n",
        "        #print (\"translated_hypo1: \", translated_hypo1.shape)\n",
        "        \n",
        "        #translated_prem = [prem sent len, batch size, hidden dim]\n",
        "        #translated_hypo = [hypo sent len, batch size, hidden dim]\n",
        "\n",
        "        translated_prem = F.relu(self.translation(embedded_prem))\n",
        "        #print (\"translated_prem: \", translated_prem.shape)\n",
        "        translated_hypo = F.relu(self.translation(embedded_hypo))\n",
        "        #print (\"translated_hypo: \", translated_hypo.shape)\n",
        "\n",
        "\n",
        "        hidden = torch.cat((translated_prem, translated_hypo), dim = 1)\n",
        "        #print (\"hidden: \", hidden.shape)\n",
        "\n",
        "        #hidden = [batch size, hid dim * 2]\n",
        "\n",
        "        hidden1 = torch.cat((translated_prem1, translated_hypo1), dim = 1)\n",
        "\n",
        "        hidden2 = torch.cat((hidden, hidden1), dim = 1)\n",
        "            \n",
        "        for fc in self.fcs:\n",
        "            hidden2 = fc(hidden2)\n",
        "            hidden2 = F.relu(hidden2)\n",
        "            hidden2 = self.dropout(hidden2)\n",
        "        #print (\"hidden: \", hidden.shape)\n",
        "        prediction = self.fc_out(hidden2)\n",
        "        #print (\"prediction: \", prediction.shape)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "      \n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM0-vYdc-rW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_DIM = 300\n",
        "FC_LAYERS = 3\n",
        "OUTPUT_DIM = len(LABEL.vocab)\n",
        "DROPOUT = 0.25\n",
        "PAD_IDX = pad_token_idx\n",
        "\n",
        "model = NLISumRNN(bert,\n",
        "              HIDDEN_DIM,\n",
        "               FC_LAYERS,\n",
        "               OUTPUT_DIM,\n",
        "               DROPOUT,\n",
        "               PAD_IDX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHkeCgaQc9xY",
        "colab_type": "code",
        "outputId": "fd016833-8a22-4d4d-ab3e-6fa068516ed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 118,953,543 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlqWvF6c-rW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plKK48D4eY7b",
        "colab_type": "code",
        "outputId": "23673af2-ae39-42f9-c0e9-5375066b160d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 9,471,303 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akr6SY9OfhCV",
        "colab_type": "code",
        "outputId": "a97271e3-27ad-4a89-fecb-19a3386e9a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "translation1.weight\n",
            "translation1.bias\n",
            "translation.weight\n",
            "translation.bias\n",
            "rnn.encoder.weight_ih_l0\n",
            "rnn.encoder.weight_hh_l0\n",
            "rnn.encoder.bias_ih_l0\n",
            "rnn.encoder.bias_hh_l0\n",
            "rnn.encoder.weight_ih_l0_reverse\n",
            "rnn.encoder.weight_hh_l0_reverse\n",
            "rnn.encoder.bias_ih_l0_reverse\n",
            "rnn.encoder.bias_hh_l0_reverse\n",
            "rnn.encoder.weight_ih_l1\n",
            "rnn.encoder.weight_hh_l1\n",
            "rnn.encoder.bias_ih_l1\n",
            "rnn.encoder.bias_hh_l1\n",
            "rnn.encoder.weight_ih_l1_reverse\n",
            "rnn.encoder.weight_hh_l1_reverse\n",
            "rnn.encoder.bias_ih_l1_reverse\n",
            "rnn.encoder.bias_hh_l1_reverse\n",
            "rnn.fc.weight\n",
            "rnn.fc.bias\n",
            "fcs.0.weight\n",
            "fcs.0.bias\n",
            "fcs.1.weight\n",
            "fcs.1.bias\n",
            "fcs.2.weight\n",
            "fcs.2.bias\n",
            "fc_out.weight\n",
            "fc_out.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy9lMDrJ-rXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mmNfj57-rXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvD-m9ib-rXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbltTB_o-rXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaGHJEUJ-rXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        prem = batch.premise\n",
        "        hypo = batch.hypothesis\n",
        "        labels = batch.label\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #prem = [prem sent len, batch size]\n",
        "        #hypo = [hypo sent len, batch size]\n",
        "        \n",
        "        predictions = model(prem, hypo)\n",
        "        \n",
        "        #predictions = [batch size, output dim]\n",
        "        #labels = [batch size]\n",
        "        \n",
        "        loss = criterion(predictions, labels)\n",
        "                \n",
        "        acc = categorical_accuracy(predictions, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjeB4J-7-rXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            prem = batch.premise\n",
        "            hypo = batch.hypothesis\n",
        "            labels = batch.label\n",
        "                        \n",
        "            predictions = model(prem, hypo)\n",
        "            \n",
        "            loss = criterion(predictions, labels)\n",
        "                \n",
        "            acc = categorical_accuracy(predictions, labels)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwJMZ19T-rXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIJfaXioFCdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPOCHS = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVuIygUp-rXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaDrOm3l-rXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fIakIy9-rXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}